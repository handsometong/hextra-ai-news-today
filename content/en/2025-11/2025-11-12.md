---
linkTitle: 11-12-Daily
title: 11-12 Daily Briefing
weight: 20
breadcrumbs: false
comments: true
description: "Polaris Alpha stealth release, InfinityStar, Doubao Seed-Code, and world-model research—Nov 12 highlights."
---

## AI News Daily · 12 Nov 2025

> `AI News` | `Daily Briefing` | `Tools` | `Research` | `Industry` | `Open Source` | `AI & Society`

### Highlights
- OpenAI quietly launches **Polaris Alpha** (widely rumoured to be GPT‑5.1) with a 256K context window and knowledge updated through Oct 2024.
- ByteDance debuts **InfinityStar**, cutting 5 s / 720p video generation to 58 seconds, while **Doubao-Seed-Code** targets agentic programming with 256K context + vision.
- Industry buzz: three chip veterans form **Majestic Labs** to build AI servers with 1000× capacity; brain–computer interfaces become a Chinese national initiative; Fei‑Fei Li calls spatial intelligence the next frontier and urges building world models.

### Product & Platform
1. **Polaris Alpha (OpenAI)** — A stealth drop with 256K context and updated knowledge; early testers show it drafting casual games straight from prompts. Community consensus: this is GPT‑5.1. [[Source]](https://www.aibase.com/zh/news/22705)  
   ![Polaris Alpha UI](https://source.hubtoday.app/images/2025/11/news_01k9sqctr3fjgaye1zm92teeek.avif)  
   ![Polaris Alpha abilities](https://source.hubtoday.app/images/2025/11/news_01k9sqcyq8fmrrct11g8r8sg83.avif)
2. **InfinityStar (ByteDance)** — A spatiotemporal pyramid model that decouples appearance/motion and leverages knowledge inheritance; renders 5 s, 720p clips in only 58 seconds. [[GitHub]](https://github.com/FoundationVision/InfinityStar)  
   ![InfinityStar architecture](https://source.hubtoday.app/images/2025/11/news_01k9sqd0xqetc98df3exgccn3v.avif)  
   ![InfinityStar samples](https://source.hubtoday.app/images/2025/11/news_01k9sqd3zfe6v8cg08zgreckjs.avif)
3. **Doubao-Seed-Code** — Doubao’s agentic coding model with 256K context plus visual comprehension of UI mockups/hand sketches, paired with new monthly pricing. [[Overview]](https://m.okjike.com/originalPosts/6912e30d0cc646ee8dac2ea0)

### Research Notes
1. **Sekai dataset** — 5,000+ hours of egocentric video from 100+ countries with scene/weather/trajectory labels, acting as a “virtual earth log” for world-model training. [[Paper]](https://arxiv.org/abs/2506.15675)
2. **FLEX paradigm** — Lets LLM agents learn from past successes and failures without retraining, improving math/chemistry tasks by up to 23%. [[Paper]](https://arxiv.org/abs/2511.06449)
3. **PDE-guided deblurring** — Introducing physics-inspired partial differential equations into the network yields visibly sharper restorations with only ~1% extra compute. [[Paper]](https://arxiv.org/abs/2511.06244)
4. **MultiSim** — Runs autonomous-driving scenarios across multiple simulators simultaneously to catch bugs that appear in the real world, not just one engine. [[Paper]](https://arxiv.org/abs/2503.08936)

### Industry / Capital
1. **Majestic Labs** — Founded by three silicon legends to pursue AI servers with “thousand-fold” memory capacity, signalling a new hardware race.
2. **Brain–computer interfaces** — Officially elevated to a Chinese national strategy; expect more funding + regulation.
3. **Fei‑Fei Li on spatial intelligence** — The Turing Award laureate argues the next wave lies in space-aware models and calls for sustained investment in world-model infrastructure.

### Community Signals
- Builders note that agentic programming (Doubao-Seed-Code) + stealth model launches (Polaris) mean longer contexts and better planning are becoming standard expectations.
